import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain import hub
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

load_dotenv()

DB_FAISS_PATH = "vectorstore/db_faiss"

# ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = FAISS.load_local(DB_FAISS_PATH, embedding, allow_dangerous_deserialization=True)

# Ø¥Ø¹Ø¯Ø§Ø¯ LLM Ù…Ù† Groq (Ø£Ù‚ÙˆÙ‰ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ø§Ù†ÙŠ Ù…Ù†Ø·Ù‚ÙŠ)
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL_NAME = "llama-3.1-70b-versatile"

llm = ChatGroq(
    model=GROQ_MODEL_NAME,
    temperature=0.1,
    max_tokens=512,
    api_key=GROQ_API_KEY
)

# Ø¨Ù†Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© RAG
prompt = hub.pull("langchain-ai/retrieval-qa-chat")
combine_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(db.as_retriever(search_kwargs={"k": 5}), combine_chain)

# Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø¤Ø§Ù„
user_query = input("ðŸ’¬ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§: ")
response = rag_chain.invoke({"input": user_query})

print("\nðŸ¤– Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
print(response["answer"])

print("\nðŸ“š Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ØµØ¯Ø±ÙŠØ©:")
for doc in response["context"]:
    print(f"- {doc.metadata} -> {doc.page_content[:200]}...")
